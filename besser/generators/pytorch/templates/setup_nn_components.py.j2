{# Template for nn components #}

{% macro add_to_set(current_set, new_element) %}
    {{ current_set }}
    {% if new_element not in current_set %}
        new_element not in current_set
        {% set updated_set = current_set + [new_element] %}
    {% else %}
        {% set updated_set = current_set %}
    {% endif %}
    {{ updated_set }}
{% endmacro %}

{%- macro initialize_dict(dict_def, key) -%}
    {%- set _ = dict_def.update({key: []}) -%}
{%- endmacro -%}

{%- macro get_loss_function(configuration) %}
    {%- if configuration.loss_function == "crossentropy" -%}
        nn.CrossEntropyLoss()
    {%- elif configuration.loss_function == "binary_crossentropy" -%}
        nn.BCELoss()
    {%- else -%}
        nn.MSELoss()
    {%- endif -%}
{% endmacro -%}

{%- macro get_optimiser(model) %}
    {%- if model.configuration.optimiser == "sgd" -%}
    {% set optimiser_name = "torch.optim.SGD" %}
    {%- elif model.configuration.optimiser == "adam" -%}
    {% set optimiser_name = "torch.optim.Adam" %}
    {%- elif model.configuration.optimiser == "adamW" -%}
    {% set optimiser_name = "torch.optim.AdamW" %}
    {%- else -%}
    {% set optimiser_name = "torch.optim.Adagrad" %}
    {%- endif -%}
    {%- if model.configuration.weight_decay != 0 and model.configuration.momentum != 0 -%}
    {{ optimiser_name }}({{ model.name }}.parameters(), lr={{ model.configuration.learning_rate }}, weight_decay={{ model.configuration.weight_decay }}, momentum={{ model.configuration.momentum }}) 
    {%- elif model.configuration.weight_decay != 0 -%}
    {{ optimiser_name }}({{ model.name }}.parameters(), lr={{ model.configuration.learning_rate }}, weight_decay={{ model.configuration.weight_decay }}) 
    {%- elif model.configuration.momentum != 0 -%}
    {{ optimiser_name }}({{ model.name }}.parameters(), lr={{ model.configuration.learning_rate }}, momentum={{ model.configuration.momentum }}) 
    {%- else -%}
    {{ optimiser_name }}({{ model.name }}.parameters(), lr={{ model.configuration.learning_rate }})
    {%- endif -%}
{% endmacro -%}

{% macro prepare_data(train_data, test_data, configuration) -%}
    {%- if train_data.input_format == "images" -%}
{%- set transformations = [] -%}

IMAGE_SIZE = ({{ train_data.image.shape[0] }}, {{ train_data.image.shape[1] }})
{%- set _ = transformations.append("transforms.Resize(IMAGE_SIZE)")%}
{%- set _ = transformations.append("transforms.ToTensor()") %}

        {%- if train_data.image.normalize %}
mean, std = compute_mean_std(r"{{ train_data.path_data }}", num_samples=100,
                             target_size=IMAGE_SIZE)
{%- set _ = transformations.append("transforms.Normalize(mean, std)")%}
        {%- endif %}
transform = transforms.Compose([
    {{ transformations | join(', \n\t') }}
    ])


# Load the training dataset
# Directory structure: root/class1/img1.jpg, root/class1/img2.jpg, root/class2/img1.jpg, ...
train_dataset = datasets.ImageFolder(root=r"{{ train_data.path_data }}", transform=transform)

# Load the testing dataset that is in a similar directory structure
test_dataset = datasets.ImageFolder(root=r"{{ test_data.path_data }}", transform=transform)

    {%- else -%}
    def load_data(csv_file):
    # Load data from CSV file
    data = pd.read_csv(csv_file)
    # Extract features and targets
    features = data.iloc[:, :-1].values.astype("float32")
    targets = data.iloc[:, -1].values.astype("float32")
    # Convert to PyTorch tensors
    features_tensor = torch.tensor(features)
    targets_tensor = torch.tensor(targets)
    # Create a TensorDataset
    dataset = torch.utils.data.TensorDataset(features_tensor, targets_tensor)
    return dataset

# Loading data
train_dataset = load_data(r"{{ train_data.path_data }}")
test_dataset = load_data(r"{{ test_data.path_data }}")
    {%- endif %}

# Create data loaders
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size={{ configuration.batch_size }}, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size={{ configuration.batch_size }}, shuffle=False)

{% endmacro -%}


{%- macro train_model(model, prediction_task) -%}
print('##### Training the model')
for epoch in range({{ model.configuration.epochs }}):
    # Initialize the running loss for the current epoch
    running_loss = 0.0
    total_loss = 0.0
    # Iterate over mini-batches of training data
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        # Zero the gradients to prepare for backward pass
        optimiser.zero_grad()
        outputs = {{ model.name }}(inputs)
        # Compute the loss 
        {%- if prediction_task == "regression" %}
        labels = labels.unsqueeze(1)
        {% elif prediction_task == "binary" %}
        outputs = outputs.squeeze()
        labels = labels.float()
        {%- endif %}
        loss = criterion(outputs, labels)
        loss.backward()
        # Update model parameters based on computed gradients
        optimiser.step()
        running_loss += loss.item()
        total_loss += loss.item()
        if i % 200 == 199:    # Print every 200 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 200))
            running_loss = 0.0
    print('[%d] overall loss for epoch: %.3f' % (epoch + 1, total_loss / len(train_loader)))
    
print('Training finished')
{%- endmacro -%}

{% macro evaluate_model(model, prediction_task) -%}
print('##### Evaluating the model')
# Disable gradient calculation during inference
with torch.no_grad():
    # Initialize lists to store predicted and true labels
    predicted_labels = []
    true_labels = []
    test_loss = 0.0
    for data in test_loader:
        # Extract inputs and labels from the data batch
        inputs, labels = data
        true_labels.extend(labels)
        # Forward pass
        outputs = {{ model.name }}(inputs)
        {% if prediction_task == "binary" -%}
        predicted = (outputs.numpy() > 0.5).astype(int)
        labels = labels.float()
        outputs = outputs.squeeze()
        {%- elif prediction_task == "multi_class" -%}
        _, predicted = torch.max(outputs.data, 1) 
        {%- else -%}
        predicted = outputs.numpy()
        labels = labels.unsqueeze(1)
        {%- endif %}
        predicted_labels.extend(predicted)
        test_loss += criterion(outputs, labels).item()

average_loss = test_loss / len(test_loader)
print(f"Test Loss: {average_loss:.3f}")

# Calculate the metrics
metrics = {{ model.configuration.metrics }}
{%- if prediction_task != "regression" %}
report = classification_report(true_labels, predicted_labels, output_dict=True)
for metric in metrics:
    {%- if prediction_task == "binary" %}
    print(f"{metric.capitalize()}: {report['1'][metric]}")
    {%- elif prediction_task == "multi_class" %}
    metric_list = []
    for class_label in report.keys():
        if class_label != "macro avg" and class_label != "weighted avg" and class_label != "accuracy":
            print(f"{metric.capitalize()} for class {class_label}: {report[class_label][metric]}")
            metric_list.append(report[class_label][metric])
    print(f"Average {metric.capitalize()}: {(sum(metric_list) / len(metric_list)):.2f}")
    print(f"Accuracy: {report['accuracy']}")
    {%- endif -%}
{%- else %}
mae = mean_absolute_error(true_labels, predicted_labels)
print(f"Mean Absolute Error (MAE): {mae}")
{%- endif -%}
{% endmacro -%}