{# Template for nn components #}

{% macro add_to_set(current_set, new_element) %}
    {{ current_set }}
    {% if new_element not in current_set %}
        new_element not in current_set
        {% set updated_set = current_set + [new_element] %}
    {% else %}
        {% set updated_set = current_set %}
    {% endif %}
    {{ updated_set }}
{% endmacro %}

{%- macro initialize_dict(dict_def, key) -%}
    {%- set _ = dict_def.update({key: []}) -%}
{%- endmacro -%}

{%- macro get_loss_function(configuration, from_logits) %}
    {%- if from_logits.flag -%}{% set fl = "from_logits=True" %}{%- else -%}{% set fl = "" %}{%- endif -%}
    {%- if configuration.loss_function == "crossentropy" -%}
        {% set loss_function = "tf.keras.losses.CategoricalCrossentropy(" ~ fl ~ ")" %}
    {%- elif configuration.loss_function == "binary_crossentropy" -%}
        {% set loss_function = "tf.keras.losses.BinaryCrossentropy(" ~ fl ~ ")" %}
    {%- else -%}
        {% set loss_function = "tf.keras.losses.MeanSquaredError()" %}
    {%- endif -%}
    {{ loss_function }}
{% endmacro -%}

{%- macro get_optimizer(model) %}
    {%- if model.configuration.optimizer == "sgd" -%}
    {% set optimizer_name = "tf.keras.optimizers.SGD" %}
    {%- elif model.configuration.optimizer == "adam" -%}
    {% set optimizer_name = "tf.keras.optimizers.Adam" %}
    {%- elif model.configuration.optimizer == "adamW" -%}
    {% set optimizer_name = "tf.keras.optimizers.AdamW" %}
    {%- else -%}
    {% set optimizer_name = "tf.keras.optimizers.Adagrad" %}
    {%- endif -%}
    {%- if model.configuration.momentum != 0 and model.configuration.optimizer == "sgd" -%}
    {%- if model.configuration.weight_decay != 0 -%}
    {{ optimizer_name }}(learning_rate={{ model.configuration.learning_rate }}, momentum={{ model.configuration.momentum }}, weight_decay={{ model.configuration.weight_decay }})
    {%- else -%}
    {{ optimizer_name }}(learning_rate={{ model.configuration.learning_rate }}, momentum={{ model.configuration.momentum }})
    {%- endif -%}
    {%- elif model.configuration.optimizer != "sgd" -%}
    {%- if model.configuration.weight_decay != 0 -%}
    {{ optimizer_name }}(learning_rate={{ model.configuration.learning_rate }}, weight_decay={{ model.configuration.weight_decay }})
    {%- else -%}
    {{ optimizer_name }}(learning_rate={{ model.configuration.learning_rate }})
    {%- endif -%}
    {%- endif -%}
{% endmacro -%}


{%- macro prepare_data(train_data, test_data, configuration) -%}
    {%- if train_data.input_format == "images" -%}
IMAGE_SIZE = ({{ train_data.image.shape[0] }}, {{ train_data.image.shape[1] }})
        {%- set resize = namespace(flag=False) %}
        {%- if train_data.image is not none -%}
            {%- if train_data.image.resize is true -%}
{% set resize = namespace(flag=True) %}
            {%- endif -%}
        {%- endif %}

# Function to load and preprocess images
{%- if train_data.image.normalize %}
scale, mean, std = compute_mean_std(r"{{ train_data.path_data }}", num_samples=100,
                                target_size=IMAGE_SIZE, resize={{ resize.flag }})
def preprocess_image(image, label, scale, mean, std):
    if scale:
        image = tf.cast(image, tf.float32) / 255.0
    image = (image - mean) / std
    return image, label

{%- else %}
scale, _, _ = compute_mean_std(r"{{ train_data.path_data }}", num_samples=100,
                                target_size=IMAGE_SIZE, resize={{ resize.flag }})
def preprocess_image(image, label, scale):
    if scale:
        image = tf.cast(image, tf.float32) / 255.0
    return image, label
{%- endif %}


# Load dataset (resizes by default)
def load_dataset(directory, mode):
    dataset = tf.keras.preprocessing.image_dataset_from_directory(
        directory=directory,
        label_mode="int",
        image_size=IMAGE_SIZE, 
        batch_size={{ configuration.batch_size }},
        shuffle=True if mode == 'train' else False,
    )
    # Apply preprocessing
    {%- if train_data.image.normalize %}
    dataset = dataset.map(lambda image, label: preprocess_image(image, label, scale, mean, std))
    {%- else %}
    dataset = dataset.map(lambda image, label: preprocess_image(image, label, scale))
    {%- endif %}
    # Prefetch for performance optimization
    AUTOTUNE = tf.data.AUTOTUNE
    dataset = dataset.prefetch(buffer_size=AUTOTUNE)
    return dataset

# Load datasets
train_loader = load_dataset(r"{{ train_data.path_data }}", "train")
test_loader = load_dataset(r"{{ test_data.path_data }}", "test")

   {%- else -%}
    def load_data(csv_file):
    # Load data from CSV file
    data = pd.read_csv(csv_file)
    # Extract features and targets
    features = data.iloc[:, :-1].values.astype("float32")
    targets = data.iloc[:, -1].values.astype("float32")
    # Convert to TensorFlow tensors
    features_tensor = tf.convert_to_tensor(features)
    targets_tensor = tf.convert_to_tensor(targets)
    # Create a TensorFlow dataset
    dataset = tf.data.Dataset.from_tensor_slices((features_tensor, targets_tensor))
    return dataset

# Load training and test data
train_dataset = load_data(r"{{ train_data.path_data }}")
test_dataset = load_data(r"{{ test_data.path_data }}")

# Create data loaders
def create_data_loader(dataset, mode):
    if mode == "train":
        dataset = dataset.shuffle(buffer_size=10000)  # Shuffle the dataset
    dataset = dataset.batch({{ configuration.batch_size }})  # Batch the dataset
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch for performance
    return dataset

# Create data loaders
train_loader = create_data_loader(train_dataset, "train")
test_loader = create_data_loader(test_dataset, "test")
{%- endif -%}
{% endmacro -%}


{%- macro train_model(model, prediction_task) -%}
print('##### Training the model')
for epoch in range({{ model.configuration.epochs }}):
    # Initialize the running loss for the current epoch
    running_loss = 0.0
    total_loss = 0.0
    # Iterate over mini-batches of training data
    for i, (inputs, labels) in enumerate(train_loader):
        with tf.GradientTape() as tape:
            outputs = {{ model.name }}(inputs, training=True)
            # Convert labels to one-hot encoding
            {% if prediction_task == "multi_class" %}
            if labels.shape.rank > 1 and labels.shape[-1] == 1:
                labels = tf.squeeze(labels, axis=-1)
            labels = tf.cast(labels, dtype=tf.int32)
            labels = tf.one_hot(labels, depth={{ model.layers[-1].out_features }})
            {% elif prediction_task == "regression" %}
            labels = tf.expand_dims(labels, axis=1)
            {% endif -%}
            loss = criterion(labels, outputs)
        # Compute gradients and update model parameters
        gradients = tape.gradient(loss, my_model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))
        total_loss += loss.numpy()
        running_loss += loss.numpy()
        if i % 200 == 199:  # Print every 200 mini-batches
            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))
            running_loss = 0.0
    print('[%d] overall loss for epoch: %.3f' % (epoch + 1, total_loss / len(train_loader)))
    total_loss = 0.0
print('Training finished')
{%- endmacro -%}


{% macro evaluate_model(model, prediction_task) -%}
print('##### Evaluating the model')
predicted_labels = []
true_labels = []
test_loss = 0.0

for inputs, labels in test_loader:
    outputs = {{ model.name }}(inputs, training=False)
    true_labels.extend(labels.numpy())
    {% if prediction_task == "binary" -%}
    predicted = (outputs.numpy() > 0.5).astype(int)
    {%- elif prediction_task == "multi_class" -%}
    predicted = tf.argmax(outputs, axis=-1).numpy()
    if labels.shape.rank > 1 and labels.shape[-1] == 1:
        labels = tf.squeeze(labels, axis=-1)
    labels = tf.cast(labels, dtype=tf.int32)
    labels = tf.one_hot(labels, depth={{ model.layers[-1].out_features }})
    {%- else -%}
    predicted = outputs.numpy()
    labels = tf.expand_dims(labels, axis=1)
    {%- endif %}
    predicted_labels.extend(predicted)
    test_loss += criterion(labels, outputs).numpy()


average_loss = test_loss / len(test_loader)
print(f"Test Loss: {average_loss:.3f}")

# Calculate the metrics
metrics = {{ model.configuration.metrics }}
{%- if prediction_task != "regression" %}
report = classification_report(true_labels, predicted_labels, output_dict=True)
for metric in metrics:
    {%- if prediction_task == "binary" %}
    print(f"{metric.capitalize()}:", report['1'][metric])
    {%- elif prediction_task == 'multi_class' %}
    metric_list = []
    for class_label in report.keys():
        if class_label != 'macro avg' and class_label != 'weighted avg' and class_label != 'accuracy':
            print(f"{metric.capitalize()} for class {class_label}:", report[class_label][metric])
            metric_list.append(report[class_label][metric])
    print(f"Average {metric.capitalize()}: {(sum(metric_list) / len(metric_list)):.2f}")
    print(f"Accuracy: {report['accuracy']}")
    {%- endif -%}
{%- else %}
mae = mean_absolute_error(true_labels, predicted_labels)
print("Mean Absolute Error (MAE):", mae)
{%- endif -%}
{% endmacro -%}